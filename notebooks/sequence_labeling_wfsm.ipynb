{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Labeling with Weighted Finite State Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Language Understanding Systems Lab\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the Laboratory Work for [Language Understanding Systems class](http://disi.unitn.it/~riccardi/page7/page13/page13.html) of [University of Trento](https://www.unitn.it/en)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Requirements__\n",
    "\n",
    "- [OpenFST](http://www.openfst.org/twiki/bin/view/FST/WebHome)\n",
    "- [OpenGRM](http://www.opengrm.org/twiki/bin/view/GRM/NGramLibrary)\n",
    "- [NL2SparQL4NLU](https://github.com/esrel/NL2SparQL4NLU) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Shallow Parsing with WFSMs: Natural Language Understanding\n",
    "\n",
    "Language Understanding of several tasks, one of which is entity extraction (concept tagging). The task is usually approached as Shallow Parsing, where we segment the input into constituents and label them using IOB-schemes.\n",
    "\n",
    "Using Weighted Finite State Machines for the task provides several benefits:\n",
    "- Even though we saw how to do sequence labeling using HMM, in real applications models can become quite complex to solve. \n",
    "- The task usually involves several components (e.g. *emission* & *transition* probabilities), and WFSMs provide an efficient way to represent and process this components via intersection and composition operations.\n",
    "    - WFSTs are good at modeling HMM and solving state machine problems\n",
    "    - Weights can be associated with edges as costs or probabilities (default: cost = negative log probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Common Sequence Labeling Pipeline\n",
    "\n",
    "The common approach to concept tagging (or sequence labeling in general) makes use of 3 components:\n",
    "\n",
    "|                   | Description                      \n",
    "|:------------------|:------------------------------\n",
    "| $$\\lambda_{W}$$   | FSA representation of an input sentence\n",
    "| $$\\lambda_{W2T}$$ | FST to translate words into output labels (e.g. `iob+type`)\n",
    "| $$\\lambda_{*LM}$$ | FSA Ngram Language Model to score the sequences of output labels\n",
    "\n",
    "Consequently, Sequence Labeling ($\\lambda$) is performed by composition of these three components as:\n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T} \\circ \\lambda_{*LM}$$\n",
    "\n",
    "- It is common to include other components to perform intermediate operations for:\n",
    "    - generalization of input ($\\lambda_{G}$)\n",
    "    - cleaning of output \n",
    "    - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. General Setup\n",
    "Let's start by preparing our workspace.\n",
    "\n",
    "- What we have is:\n",
    "    - training & test sets in utterance-per-line format\n",
    "    - training & test sets in CoNLL format that contain word-tag observations\n",
    "    \n",
    "- To work with this data we need functions to:\n",
    "    - apply frequency cut-offs to handle OOV\n",
    "    - reading CoNLL format corpus for processing\n",
    "\n",
    "- For working with WFSMs we need:\n",
    "    - input symbol table: words\n",
    "    - output symbol table: tags\n",
    "    - data in utterance-per-line & CoNLL formats\n",
    "    \n",
    "- Our main mechanism for OOV Handling will be frequency cut-off on lexicon, replacement of OOV in training and testing data will be done using __command-line tools__.\n",
    "- We will be also *extensively* writing our own FSMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Python Functions for Corpus and Lexicon Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Corpus Reading: Utterance-per-line Format (from Lab on Ngram Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_file):\n",
    "    \"\"\"\n",
    "    read corpus into a list-of-lists, splitting sentences into tokens by space (' ')\n",
    "    :param corpus_file: corpus file in sentence-per-line format (tokenized)\n",
    "    \"\"\"\n",
    "    return [line.strip().split() for line in open(corpus_file, 'r')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CoNLL Corpus Reading (from Lab on Sequence Labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus_conll(corpus_file, fs=\"\\t\"):\n",
    "    \"\"\"\n",
    "    read corpus in CoNLL format\n",
    "    :param corpus_file: corpus in conll format\n",
    "    :param fs: field separator\n",
    "    :return: corpus\n",
    "    \"\"\"\n",
    "    featn = None  # number of features for consistency check\n",
    "    sents = []  # list to hold words list sequences\n",
    "    words = []  # list to hold feature tuples\n",
    "\n",
    "    for line in open(corpus_file):\n",
    "        line = line.strip()\n",
    "        if len(line.strip()) > 0:\n",
    "            feats = tuple(line.strip().split(fs))\n",
    "            if not featn:\n",
    "                featn = len(feats)\n",
    "            elif featn != len(feats) and len(feats) != 0:\n",
    "                raise ValueError(\"Unexpected number of columns {} ({})\".format(len(feats), featn))\n",
    "\n",
    "            words.append(feats)\n",
    "        else:\n",
    "            if len(words) > 0:\n",
    "                sents.append(words)\n",
    "                words = []\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Utility function to get labels stripped of IOB\n",
    "def parse_iob(t):\n",
    "    m = re.match(r'^([^-]*)-(.*)$', t)\n",
    "    return m.groups() if m else (t, None)\n",
    "\n",
    "def get_chunks(corpus_file, fs=\"\\t\", otag=\"O\"):\n",
    "    sents = read_corpus_conll(corpus_file, fs=fs)\n",
    "    return set([parse_iob(token[-1])[1] for sent in sents for token in sent if token[-1] != otag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function to simplify working with data\n",
    "# get column from loaded corpus (tokens are tuples)\n",
    "def get_column(corpus, column=-1):\n",
    "    return [[word[column] for word in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequency Cut-Off using Corpus (from Lab on Ngram Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency_list(corpus):\n",
    "    \"\"\"\n",
    "    create frequency list for a corpus\n",
    "    :param corpus: corpus as list of lists\n",
    "    \"\"\"\n",
    "    frequencies = {}\n",
    "    for sent in corpus:\n",
    "        for token in sent:\n",
    "            frequencies[token] = frequencies.setdefault(token, 0) + 1\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff(corpus, tf_min=2):\n",
    "    \"\"\"\n",
    "    apply min cutoffs\n",
    "    :param tf_min: minimum token frequency for lexicon elements (below removed); default 2\n",
    "    :return: lexicon as set\n",
    "    \"\"\"\n",
    "    frequencies = compute_frequency_list(corpus)\n",
    "    return sorted([token for token, frequency in frequencies.items() if frequency >= tf_min])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation (from Lab on Sequence Labeling)\n",
    "- For evaluation we are going to use `conll.py`'s `evaluate` (in CoNLL eval style)\n",
    "- Results will be reported using `pandas` Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to import conll\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "\n",
    "from conll import evaluate\n",
    "# for nice tables\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Setting Up..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing Input Symbol Tables (`isyms.txt`)\n",
    "- Since we will be using corpus files a lot, let's copy them into current directory with shorter names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dpath='NL2SparQL4NLU/dataset/NL2SparQL4NLU'\n",
    "\n",
    "cp $dpath.train.utterances.txt trn.txt\n",
    "cp $dpath.test.utterances.txt tst.txt\n",
    "\n",
    "cp $dpath.train.conll.txt trn.conll\n",
    "cp $dpath.test.conll.txt tst.conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create symbol tables for our data\n",
    "    - apply cut-off using our functions\n",
    "    - create symbol table using `ngramsymbols`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = read_corpus('trn.txt')\n",
    "trn_lex = cutoff(trn_data)\n",
    "\n",
    "with open('isyms.trn.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(trn_lex) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ngramsymbols isyms.trn.txt isyms.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# let's compile both training and test set into far using this symbol table\n",
    "farcompilestrings \\\n",
    "    --symbols=isyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    trn.txt trn.far\n",
    "\n",
    "farcompilestrings \\\n",
    "    --symbols=isyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    tst.txt tst.far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we have:\n",
    "- Symbol table (`isyms.txt`)\n",
    "    - contains `['<s>', '</s>', '<epsilon>', '<unk>']` that are added automatically\n",
    "- Training data as FAR with OOV replaced (`trn.far`)\n",
    "- Test data as FAR with OOV replaced (`tst.far`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating Output Symbol Table (`osyms.txt`)\n",
    "To do sequence labeling we additionally require *output symbol table*.\n",
    "\n",
    "In case we know the concepts (__types__), we can build our output symbol table without looking at data.\n",
    "\n",
    "Since our output labels are composed of segmentation (`iob`) and classification (`type`) labels, we can make sure that each __type__ has all possible IOB prefixes.\n",
    "\n",
    "- prefix each __type__ with all possible IOB prefixes (i.e. `I-` and `B-`)\n",
    "- generate symbol table using `ngramsymbols` (as shown above)\n",
    "\n",
    "In case we don't know the list of __types__, we have to extract __types__ from training data in CoNLL format (stripping `iob` prefix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique list of types\n",
    "types = get_chunks('trn.conll')\n",
    "\n",
    "with open('osyms.u.lst.txt', 'w') as f:\n",
    "    # let's add 'O'\n",
    "    f.write(\"O\" + \"\\n\")\n",
    "    for c in sorted(list(types)):\n",
    "        # prefix each type with segmentation information\n",
    "        f.write(\"B-\"+ c + \"\\n\")\n",
    "        f.write(\"I-\"+ c + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# generate output symbol table with iob-prefixed typed\n",
    "ngramsymbols osyms.u.lst.txt osyms.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Applying FSTs\n",
    "- Our objective is to be able to annotate input sentences (as FSAs in FAR) using the machines we are going to build. \n",
    "- Our test data has been loaded into FAR archive\n",
    "- We will can `farextract` to extract these sentence FSAs and apply our FSMs to them\n",
    "    - `farextract --filename_prefix=\"<odir>\" <FAR>` will extract contents of `<FAR>` into directory `<odir>`\n",
    "    - we can iterate over files in the directory and apply operations (see below)\n",
    "- We can also create FAR of the processed FSMs using `farcreate` as\n",
    "    - `farcreate --file_list_input <list of FST filenames> <output FAR>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tstar\n",
      "1\t2\tof\tof\n",
      "2\t3\t<unk>\t<unk>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wdir='wdir'\n",
    "mkdir -p $wdir\n",
    "\n",
    "farextract --filename_prefix=\"$wdir/\" tst.far\n",
    "cp $wdir/tst.txt-0001 sent.fsa\n",
    "\n",
    "fstprint sent.fsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For testing we are going to use this `sent.fsa`\n",
    "- For evaluation we are going to iterate over whole FAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Baselines\n",
    "Let's demonstrate the process by building some simple Shallow Parsing models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Random\n",
    "The simplest solution is to assign output labels __randomly__. \n",
    "\n",
    "To achieve this we need to:\n",
    "- implement an FST that translates our words into output symbols ($\\lambda_{W2T}$) with equal cost, or no cost at all (i.e. unweighted FST)\n",
    "    - let's call it $\\lambda_{W2T_{U}}$ for [universe](https://en.wikipedia.org/wiki/Universe_(mathematics)).\n",
    "- compose it with our sentence\n",
    "- choose a random path in FST\n",
    "\n",
    "The FST $\\lambda_{W2T_{U}}$ represents search space for $p(w_i|t_i)$ without being exposed to any observation. It is build using only our knowledge of the __domain__:\n",
    "- vocabulary of language (input symbols)\n",
    "- concepts __types__ in our domain\n",
    "\n",
    "input and output symbols and all translations are possible. \n",
    "\n",
    "Since we have no model yet, the whole pipeline is:\n",
    "\n",
    "$$\\lambda_{R} = \\lambda_{W} \\circ \\lambda_{W2T_{U}}$$\n",
    "\n",
    "- __*random path*__ here is opposed to __*best path*__ or __*shortest path*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's define a function in python to write FST specification given input and output symbol tables as below\n",
    "    - we will be using it a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_w2t(isyms, osyms, out='w2t.tmp'):\n",
    "    special = {'<epsilon>', '<s>', '</s>'}\n",
    "    oov = '<unk>'  # unknown symbol\n",
    "    state = '0'    # wfst specification state\n",
    "    fs = \" \"       # wfst specification column separator\n",
    "    \n",
    "    ist = sorted(list(set([line.strip().split(\"\\t\")[0] for line in open(isyms, 'r')]) - special))\n",
    "    ost = sorted(list(set([line.strip().split(\"\\t\")[0] for line in open(osyms, 'r')]) - special))\n",
    "    \n",
    "    with open(out, 'w') as f:\n",
    "        for i in range(len(ist)):\n",
    "            for j in range(len(ost)):\n",
    "                f.write(fs.join([state, state, ist[i], ost[j]]) + \"\\n\")\n",
    "        f.write(state + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_w2t('isyms.txt', 'osyms.txt', out='w2t_u.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst type                                          vector\n",
      "arc type                                          standard\n",
      "input symbol table                                isyms.txt\n",
      "output symbol table                               osyms.txt\n",
      "# of states                                       1\n",
      "# of arcs                                         45648\n",
      "initial state                                     0\n",
      "# of final states                                 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Let's compile it\n",
    "fstcompile \\\n",
    "    --isymbols=isyms.txt \\\n",
    "    --osymbols=osyms.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    w2t_u.txt w2t_u.bin\n",
    "\n",
    "fstinfo w2t_u.bin | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Compute input and output symbol table sizes\n",
    "- Compare their multiplication to `# of arcs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "- note the usage of `fstrandgen` instead of `fstshortestpath` to get __random__ paths of FST.\n",
    "- All tokens will be predicted as `O`, if we use `fstshortestpath`.\n",
    "    - Bonus Question: *Why?* (try uncommenting & running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tB-actor.type\n",
      "1\t2\tof\tI-actor.nationality\n",
      "2\t3\t<unk>\tB-country.name\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t_u.bin | fstrandgen | fstrmepsilon | fsttopsort | fstprint --isymbols=isyms.txt\n",
    "# fstcompose sent.fsa w2t_u.bin | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "- For evaluation we are going to use `conll.py`'s `evaluate` function (provided is `src` directory)\n",
    "- We first will collect predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collecting prediction from our model & storing them into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wdir='wdir'\n",
    "farr=($(ls $wdir))\n",
    "\n",
    "for f in ${farr[@]}\n",
    "do\n",
    "    fstcompose $wdir/$f w2t_u.bin | fstrandgen | fstrmepsilon | fsttopsort | fstprint --isymbols=isyms.txt\n",
    "done > w2t_u.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- below is the function to process this output and read it for evaluation: `read_fst4conll`\n",
    "    - additionally it substitutes `<unk>` in output labels with '`O`', for function to work (there is no `<unk>` in IOB scheme)\n",
    "- it is the modified version of `read_corpus_conll` which we will use to load references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version to support fst-output\n",
    "def read_fst4conll(fst_file, fs=\"\\t\", oov='<unk>', otag='O', sep='+', split=False):\n",
    "    \"\"\"\n",
    "    :param corpus_file: corpus in conll format\n",
    "    :param fs: field separator\n",
    "    :param oov: token to map to otag (we need to get rid of <unk> in labels)\n",
    "    :param otag: otag symbol\n",
    "    :param sep: \n",
    "    :param split:\n",
    "    :return: corpus \n",
    "    \"\"\"\n",
    "    sents = []  # list to hold words list sequences\n",
    "    words = []  # list to hold feature tuples\n",
    "\n",
    "    for line in open(fst_file):\n",
    "        line = line.strip()\n",
    "        if len(line.strip()) > 0:\n",
    "            feats = tuple(line.strip().split(fs))\n",
    "            # arc has minimum 3 columns, else final state\n",
    "            if len(feats) >= 3:\n",
    "                ist = feats[2]  # 3rd column (input)\n",
    "                ost = feats[3]  # 4th column (output)\n",
    "                # replace '<unk>' with 'O'\n",
    "                ost = otag if ost == oov else ost\n",
    "                # ignore for now\n",
    "                ost = ost.split(sep)[1] if split and ost != otag else ost\n",
    "                \n",
    "                words.append((ist, ost))\n",
    "            else:\n",
    "                sents.append(words)\n",
    "                words = []\n",
    "        else:\n",
    "            if len(words) > 0:\n",
    "                sents.append(words) \n",
    "                words = []\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>country.name</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.011</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.location</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>award.ceremony</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.007</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>award.category</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director.nationality</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>character.name</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.006</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.description</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.gross_revenue</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.genre</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.007</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor.nationality</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.type</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.release_region</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.language</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.032</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating.name</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.018</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.star_rating</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>producer.name</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.release_date</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor.name</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor.type</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.subject</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director.name</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person.nationality</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person.name</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.name</th>\n",
       "      <td>0.026</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.019</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>1091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          p      r      f     s\n",
       "country.name          0.007  0.032  0.011    62\n",
       "movie.location        0.000  0.000  0.000     7\n",
       "award.ceremony        0.004  0.143  0.007     7\n",
       "award.category        0.000  0.000  0.000     2\n",
       "director.nationality  0.000  0.000  0.000     1\n",
       "character.name        0.003  0.067  0.006    15\n",
       "movie.description     0.000  0.000  0.000     0\n",
       "movie.gross_revenue   0.000  0.000  0.000     5\n",
       "movie.genre           0.004  0.028  0.007    36\n",
       "actor.nationality     0.000  0.000  0.000     1\n",
       "movie.type            1.000  0.000  0.000     4\n",
       "movie.release_region  0.000  0.000  0.000     4\n",
       "movie.language        0.020  0.087  0.032    69\n",
       "rating.name           0.011  0.049  0.018    61\n",
       "movie.star_rating     0.000  0.000  0.000     1\n",
       "producer.name         0.003  0.014  0.005    73\n",
       "movie.release_date    0.000  0.000  0.000    29\n",
       "actor.name            0.000  0.000  0.000    80\n",
       "actor.type            0.000  0.000  0.000     2\n",
       "movie.subject         0.000  0.000  0.000    44\n",
       "director.name         0.000  0.000  0.000    81\n",
       "person.nationality    0.000  0.000  0.000     0\n",
       "person.name           0.000  0.000  0.000    34\n",
       "movie.name            0.026  0.015  0.019   473\n",
       "total                 0.003  0.020  0.006  1091"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs = read_corpus_conll('tst.conll')\n",
    "hyps = read_fst4conll('w2t_u.out')\n",
    "\n",
    "results = evaluate(refs, hyps)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Output Symbol Priors\n",
    "\n",
    "The simplest form for the third component is to use output label priors, i.e. unigram probabilities of output labels. To model that, we can:\n",
    "- train a unigram language model using `ngramcount` & `ngrammake` (let's call it $\\lambda_{LM_{1}}$)\n",
    "- compose it with the $\\lambda_{W2T_{U}}$ so that the whole pipeline becomes:\n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{U}} \\circ \\lambda_{LM_{1}}$$\n",
    "\n",
    "- Since `O` tag is the most frequent & we will have a model that always predicts it.\n",
    "- We can represent our model as:\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(t_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Considerations__\n",
    "- In this baseline we model __observations__. Due to the fact that:\n",
    "    - our models are based on observations\n",
    "    - OOV is due to scarcity of observations\n",
    "- We need to change output symbol table of $\\lambda_{W2T_{U}}$ to output only tags present in data or `<unk>`. \n",
    "    - it might happen such that an `iob+type` combination never appears in our training data\n",
    "    - output symbol tables of $\\lambda_{W2T}$ (FST) and symbol table of $\\lambda_{LM_{1}}$ (FSA) have to match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Training\" a Model\n",
    "A new $\\lambda_{*LM}$ model is built following these steps:\n",
    "1. prepare training data for model (in required format)\n",
    "2. prepare symbol table for that data\n",
    "    - apply OOV handing (you can use any of the approaches to introduce `<unk>`)\n",
    "3. compile training data into FAR using this symbol table\n",
    "4. estimate model probabilities for $\\lambda_{*LM}$ (i.e. train ngram model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new $\\lambda_{W2T}$ is created (updated) each time we change the symbol table of the $\\lambda_{*LM}$.\n",
    "\n",
    "- If we do not plan to estimate $p(w_i|t_i)$ in $\\lambda_{W2T}$, we can create the FST as we did for $\\lambda_{W2T_{U}}$ (and keeping input symbol table the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data in utterance-per-line format for output symbols (t - tags)\n",
    "trn = read_corpus_conll('trn.conll')\n",
    "tags = get_column(trn, column=-1)\n",
    "\n",
    "# write data\n",
    "with open('trn.t.txt', 'w') as f:\n",
    "    for s in tags:\n",
    "        f.write(\" \".join(s) + \"\\n\")\n",
    "        \n",
    "tlex = cutoff(tags)\n",
    "with open('osyms.t.lst.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(tlex) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# make symbol table\n",
    "ngramsymbols osyms.t.lst.txt osyms.t.txt\n",
    "# compile data into FAR again\n",
    "farcompilestrings \\\n",
    "    --symbols=osyms.t.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    trn.t.txt trn.t.far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's train a unigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of states                                       1\n",
      "# of ngram arcs                                   38\n",
      "# of backoff arcs                                 0\n",
      "initial state                                     0\n",
      "unigram state                                     -1\n",
      "# of final states                                 1\n",
      "ngram order                                       1\n",
      "# of 1-grams                                      39\n",
      "well-formed                                       y\n",
      "normalized                                        y\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ngramcount --order=1 trn.t.far trn.t1.cnt\n",
    "ngrammake trn.t1.cnt t1.lm\n",
    "ngraminfo t1.lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create a new $\\lambda_{W2T}$ (let's call it $\\lambda_{W2T_{T}}$ for \"tags\"):\n",
    "    - following the same procedure we followed for $\\lambda_{W2T_{U}}$, but using:\n",
    "        - as input symbol table (`isyms.txt`)\n",
    "        - as output symbol table (`t.osyms.txt`)\n",
    "    - allowing `<unk> <unk>` and *word*-`<unk>` arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_w2t('isyms.txt', 'osyms.t.txt', out='w2t_t.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst type                                          vector\n",
      "arc type                                          standard\n",
      "input symbol table                                isyms.txt\n",
      "output symbol table                               osyms.t.txt\n",
      "# of states                                       1\n",
      "# of arcs                                         36138\n",
      "initial state                                     0\n",
      "# of final states                                 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Let's compile it\n",
    "fstcompile \\\n",
    "    --isymbols=isyms.txt \\\n",
    "    --osymbols=osyms.t.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    w2t_t.txt w2t_t.bin\n",
    "\n",
    "fstinfo w2t_t.bin | head -n 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tO\t0.476697922\n",
      "1\t2\tof\tO\t0.476697922\n",
      "2\t3\t<unk>\tO\t0.476697922\n",
      "3\t2.00510979\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t_t.bin | fstcompose - t1.lm | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wdir='wdir'\n",
    "farr=($(ls $wdir))\n",
    "\n",
    "for f in ${farr[@]}\n",
    "do\n",
    "    fstcompose $wdir/$f w2t_t.bin | fstcompose - t1.lm |\\\n",
    "        fstshortestpath | fstrmepsilon | fsttopsort | fstprint --isymbols=isyms.txt\n",
    "done > w2t_t.t1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>country.name</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.location</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>award.ceremony</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>award.category</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director.nationality</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>character.name</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.gross_revenue</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.genre</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor.nationality</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.type</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.release_region</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.language</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating.name</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.star_rating</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>producer.name</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.release_date</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor.name</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor.type</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.subject</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director.name</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person.name</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.name</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      p    r    f     s\n",
       "country.name          1  0.0  0.0    62\n",
       "movie.location        1  0.0  0.0     7\n",
       "award.ceremony        1  0.0  0.0     7\n",
       "award.category        1  0.0  0.0     2\n",
       "director.nationality  1  0.0  0.0     1\n",
       "character.name        1  0.0  0.0    15\n",
       "movie.gross_revenue   1  0.0  0.0     5\n",
       "movie.genre           1  0.0  0.0    36\n",
       "actor.nationality     1  0.0  0.0     1\n",
       "movie.type            1  0.0  0.0     4\n",
       "movie.release_region  1  0.0  0.0     4\n",
       "movie.language        1  0.0  0.0    69\n",
       "rating.name           1  0.0  0.0    61\n",
       "movie.star_rating     1  0.0  0.0     1\n",
       "producer.name         1  0.0  0.0    73\n",
       "movie.release_date    1  0.0  0.0    29\n",
       "actor.name            1  0.0  0.0    80\n",
       "actor.type            1  0.0  0.0     2\n",
       "movie.subject         1  0.0  0.0    44\n",
       "director.name         1  0.0  0.0    81\n",
       "person.name           1  0.0  0.0    34\n",
       "movie.name            1  0.0  0.0   473\n",
       "total                 1  0.0  0.0  1091"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs = read_corpus_conll('tst.conll')\n",
    "hyps = read_fst4conll('w2t_t.t1.out')\n",
    "\n",
    "results = evaluate(refs, hyps)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model still has $F_1=0$, since `O` is the tag with highest prior.\n",
    "- Observe the weights in the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. Exercises\n",
    "- Compare sizes of $\\lambda_{W2T_{U}}$ and $\\lambda_{W2T_{T}}$ for `# of arcs`\n",
    "- Unigram models & $\\lambda_{W2T}$\n",
    "    - Test pipeline: $\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{U}} \\circ \\lambda_{LM_{1}}$\n",
    "\n",
    "\n",
    "- Bigram models: train a *tag* bigram model (let's call it $\\lambda_{LM_{2}}$)\n",
    "\n",
    "    - Test pipeline with $\\lambda_{W2T_{U}}$: $\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{U}} \\circ \\lambda_{LM_{2}}$\n",
    "    - Test pipeline with $\\lambda_{W2T_{T}}$: $\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{T}} \\circ \\lambda_{LM_{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5. Maximum Likelihood Estimation (Emission Probabilities)\n",
    "- So far we haven't explored the relation between input and output\n",
    "- The next thing we can do is to expose our model to observations and estimate $p(w_{i}|t_{i})$ from data.\n",
    "- We can use `ngramcount` and `ngrammake` to make a smoothed probability model (we are using default, i.e. no parameters). \n",
    "- We need to estimate probabilities like we would estimate bigram probabilities, thus:\n",
    "    - prepare lexicon with *tags* and *words*\n",
    "    - read CoNLL format corpus into far (token per line, preprocessed)\n",
    "    - count bigrams\n",
    "    - make a bigram language model\n",
    "    - print bigrams with weights (negative log probabilities)\n",
    "    - choose bigrams (it will contain unigrams, as well as `<s>` and `</s>` bigrams)\n",
    "    - convert to FST & compile\n",
    "    \n",
    "- Let's call the model $\\lambda_{W2T_{MLE}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# lets use our symbol tables (since they both have been applied cut-off)\n",
    "cat isyms.txt osyms.t.txt | cut -f 1 | sort | uniq > msyms.m.lst.txt\n",
    "ngramsymbols msyms.m.lst.txt msyms.t.txt\n",
    "\n",
    "# let's convert data to ngrams\n",
    "cat trn.conll | sed '/^$/d' | awk '{print $2,$1}' > trn.w2t.txt\n",
    "\n",
    "# compile to far\n",
    "farcompilestrings \\\n",
    "    --symbols=msyms.t.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    trn.w2t.txt trn.w2t.far\n",
    "    \n",
    "# count bigrams\n",
    "ngramcount --order=2 trn.w2t.far trn.w2t.cnt\n",
    "# make a model\n",
    "ngrammake trn.w2t.cnt trn.w2t.lm\n",
    "\n",
    "# print ngram probabilities as negative logs\n",
    "ngramprint \\\n",
    "    --symbols=msyms.t.txt\\\n",
    "    --negativelogs \\\n",
    "    trn.w2t.lm trn.w2t.probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's define a python function to convert probabilities printout to W2T FST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_w2t_mle(probs, out='w2t_mle.tmp'):\n",
    "    special = {'<epsilon>', '<s>', '</s>'}\n",
    "    oov = '<unk>'  # unknown symbol\n",
    "    state = '0'    # wfst specification state\n",
    "    fs = \" \"       # wfst specification column separator\n",
    "    otag = 'O'\n",
    "    mcn = 3        # minimum column number\n",
    "    \n",
    "    lines = [line.strip().split(\"\\t\") for line in open(probs, 'r')]\n",
    "\n",
    "    with open(out, 'w') as f:\n",
    "        for line in lines:\n",
    "            ngram = line[0]\n",
    "            ngram_words = ngram.split()  # by space\n",
    "            if len(ngram_words) == 2:\n",
    "                if set(ngram_words).isdisjoint(set(special)):\n",
    "                    if ngram_words[0] in [otag, oov]:\n",
    "                        f.write(fs.join([state, state] + ngram_words + [line[1]]) + \"\\n\")\n",
    "                    elif ngram_words[0].startswith(\"B-\") or ngram_words[0].startswith(\"I-\"):\n",
    "                        f.write(fs.join([state, state] + line) + \"\\n\")\n",
    "        f.write(state + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_w2t_mle('trn.w2t.probs', out='trn.w2t_mle.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst type                                          vector\n",
      "arc type                                          standard\n",
      "input symbol table                                isyms.txt\n",
      "output symbol table                               osyms.t.txt\n",
      "# of states                                       1\n",
      "# of arcs                                         1513\n",
      "initial state                                     0\n",
      "# of final states                                 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompile \\\n",
    "    --isymbols=osyms.t.txt \\\n",
    "    --osymbols=isyms.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    trn.w2t_mle.txt w2t_mle.bin\n",
    "    \n",
    "# we need to invert it to have words on input\n",
    "fstinvert w2t_mle.bin w2t_mle.inv.bin\n",
    "\n",
    "fstinfo w2t_mle.inv.bin | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tB-movie.name\t3.22130394\n",
      "1\t2\tof\tI-movie.name\t3.02857399\n",
      "2\t3\t<unk>\tB-director.nationality\t0.694147706\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t_mle.inv.bin | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The pipeline above represents \n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)}$$\n",
    "\n",
    "- To extend it to unigram tagging model we need to compose it with  $\\lambda_{LM_{1}} = p(t_i)$ \n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{MLE}} \\circ \\lambda_{LM_{1}}$$\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)p(t_i)}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tB-movie.name\t6.09388542\n",
      "1\t2\tof\tO\t3.86926198\n",
      "2\t3\t<unk>\tO\t4.46324492\n",
      "3\t2.00510979\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t_mle.inv.bin | fstcompose - t1.lm | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Maximum Likelihood Estimation\n",
    "- using `ngramprint` verify the Maximum Likelihood Estimation method (without `--negativelogs` it prints raw probabilities)\n",
    "    - print bigram counts from $\\lambda_{W2T_{MLE}}$ (output of `ngramcount`)\n",
    "    - print unigram counts for either from $\\lambda_{LM_{1}}$ or $\\lambda_{W2T_{MLE}}$ (output of `ngramcount`)\n",
    "    - using these counts compute probability of $p($ `brad|B-actor.name` $)$\n",
    "    - extract probability of $p($ `brad|B-actor.name` $)$ from $\\lambda_{W2T_{MLE}}$ (output of `ngrammake`)\n",
    "    - compare values\n",
    "    - repeat the procedure using counts from methods developed for the lab on ngram modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Markov Model Tagger\n",
    "- Evaluate the MLE pipeline using bigram model on tags, i.e.\n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{MLE}} \\circ \\lambda_{LM_{2}}$$\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)p(t_i|t_{i-1})}$$ \n",
    "\n",
    "- compare performances to the HMM tagger from previous lab (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Joint Distribution Modeling\n",
    "\n",
    "As we have seen, sequence labeling for Language Understanding could be approached using Hidden Markov Models (similar to Part-of-Speech Tagging), and to models it as in the table below (__HMM__). Stochastic Conceptual Language Models for Spoken Language Understanding in [Raymond & Riccardi (2007)](https://disi.unitn.it/~riccardi/papers2/IS07-GenerDiscrSLU.pdf) (__R&R__) model it jointly.\n",
    "\n",
    "\n",
    "| Model   | Equation |\n",
    "|:--------|:----------\n",
    "| __HMM__ | $$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)p(t_i|t_{i-N+1}^{i-1})}$$\n",
    "| __R&R__ | $$p(w_{1}^n,t_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_{i}t_{i}|w_{i-N+1}^{i-1}t_{i-N+1}^{i-1})}$$\n",
    "\n",
    "\n",
    "From implementation perspective, joint modeling implies the following:\n",
    "- we need to train $\\lambda_{SCLM}$ on word-tag pairs\n",
    "    - create corpus in a format for estimating $p(w_i,t_i|w_{i-N+1}^{i-1}t_{i-N+1}^{i-1})$\n",
    "    - create symbol tables\n",
    "- we need to change $\\lambda_{W2T}$ to output *word-tag* pairs (let's call it $\\lambda_{W2WT}$)\n",
    "    - create FST like above for $\\lambda_{W2WT}$ ($\\lambda_{W2WT_{WT}}$ - to differentiate from $\\lambda_{W2WT_{U}}$ that contains all possible combinations)\n",
    "- we also need to change our input symbol table to accommodate OOV words due to joint modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Symbol Tables\n",
    "- Let's create output symbol table the same way we did for $\\lambda_{W2T_{T}}$\n",
    "- Let's create input symbol taking $w$ from the $w,t$ pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data in utterance-per-line format for output symbols (w+t)\n",
    "trn = read_corpus_conll('trn.conll')\n",
    "wt_sents = [[\"+\".join(w) for w in s] for s in trn]\n",
    "wt_osyms = cutoff(wt_sents)\n",
    "wt_isyms = [w.split('+')[0] for w in wt_osyms]\n",
    "\n",
    "with open('trn.wt.txt', 'w') as f:\n",
    "    for s in wt_sents:\n",
    "        f.write(\" \".join(s) + \"\\n\")\n",
    "        \n",
    "with open('osyms.wt.lst.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(wt_osyms) + \"\\n\")\n",
    "    \n",
    "with open('isyms.wt.lst.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(wt_isyms) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ngramsymbols osyms.wt.lst.txt osyms.wt.txt\n",
    "ngramsymbols isyms.wt.lst.txt isyms.wt.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's:\n",
    "    - compile our processed data into FAR\n",
    "    - train ngram language models on it - $\\lambda_{SCLM}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Conceptual Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of states                                       1096\n",
      "# of ngram arcs                                   6179\n",
      "# of backoff arcs                                 1095\n",
      "initial state                                     1\n",
      "unigram state                                     0\n",
      "# of final states                                 533\n",
      "ngram order                                       2\n",
      "# of 1-grams                                      1095\n",
      "# of 2-grams                                      5617\n",
      "well-formed                                       y\n",
      "normalized                                        y\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# compile data into FAR\n",
    "farcompilestrings \\\n",
    "    --symbols=osyms.wt.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    trn.wt.txt trn.wt.far\n",
    "\n",
    "# train ngram model\n",
    "ngramcount --order=2 trn.wt.far trn.wt.cnt\n",
    "ngrammake trn.wt.cnt wt2.lm\n",
    "ngraminfo wt2.lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building W2WT FST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's build unweighted $\\lambda_{W2WT_{WT}}$, using\n",
    "    - input symbol table `isyms.wt.txt`\n",
    "    - output symbol table `osyms.wt.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_w2t_wt(isyms, sep='+', out='w2wt.tmp'):\n",
    "    special = {'<epsilon>', '<s>', '</s>'}\n",
    "    oov = '<unk>'  # unknown symbol\n",
    "    state = '0'    # wfst specification state\n",
    "    fs = \" \"       # wfst specification column separator\n",
    "    \n",
    "    ist = sorted(list(set([line.strip().split(\"\\t\")[0] for line in open(isyms, 'r')]) - special))\n",
    "    \n",
    "    with open(out, 'w') as f:\n",
    "        for e in ist:\n",
    "            f.write(fs.join([state, state, e.split('+')[0], e]) + \"\\n\")\n",
    "        f.write(state + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_w2t_wt('osyms.wt.txt', out='w2wt_wt.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst type                                          vector\n",
      "arc type                                          standard\n",
      "input symbol table                                isyms.wt.txt\n",
      "output symbol table                               osyms.wt.txt\n",
      "# of states                                       1\n",
      "# of arcs                                         1094\n",
      "initial state                                     0\n",
      "# of final states                                 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Let's compile it\n",
    "fstcompile \\\n",
    "    --isymbols=isyms.wt.txt \\\n",
    "    --osymbols=osyms.wt.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    w2wt_wt.txt w2wt_wt.bin\n",
    "\n",
    "fstinfo w2wt_wt.bin | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets test the whole $\\lambda_{W} \\circ \\lambda_{W2WT_{WT}} \\circ \\lambda_{SCLM_{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Test Data\n",
    "- since we have changed input symbol table we need to recompile & extract our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tstar\n",
      "1\t2\tof\tof\n",
      "2\t3\t<unk>\t<unk>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "farcompilestrings \\\n",
    "    --symbols=isyms.wt.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    tst.txt tst.wt.far\n",
    "\n",
    "wdir='wdir_wt'\n",
    "mkdir -p $wdir\n",
    "\n",
    "farextract --filename_prefix=\"$wdir/\" tst.wt.far\n",
    "cp $wdir/tst.txt-0001 sent.wt.fsa\n",
    "\n",
    "fstprint sent.wt.fsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tstar+O\t7.93891811\n",
      "1\t2\tof\tof+O\t1.55421352\n",
      "2\t3\t<unk>\t<unk>\t2.84977818\n",
      "3\t1.10391009\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.wt.fsa w2wt_wt.bin | fstcompose - wt2.lm | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "- Since on the output we have `word+tag`, we need to post-process the output for evaluation\n",
    "- the function `read_fst4conll` already has that functionality via `split=True` and `sep='+'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wdir='wdir_wt'\n",
    "farr=($(ls $wdir))\n",
    "\n",
    "for f in ${farr[@]}\n",
    "do\n",
    "    fstcompose $wdir/$f w2wt_wt.bin | fstcompose - wt2.lm |\\\n",
    "        fstshortestpath | fstrmepsilon | fsttopsort | fstprint --isymbols=isyms.wt.txt\n",
    "done > w2wt_wt.wt2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>country.name</th>\n",
       "      <td>0.629</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.667</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.location</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>award.ceremony</th>\n",
       "      <td>0.714</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.714</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>award.category</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director.nationality</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>character.name</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.381</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.gross_revenue</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.genre</th>\n",
       "      <td>0.963</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.825</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor.nationality</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.type</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.release_region</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.language</th>\n",
       "      <td>0.786</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.704</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating.name</th>\n",
       "      <td>0.935</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.943</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.star_rating</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>producer.name</th>\n",
       "      <td>0.692</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.652</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.release_date</th>\n",
       "      <td>0.412</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.444</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor.name</th>\n",
       "      <td>0.730</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.701</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor.type</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.subject</th>\n",
       "      <td>0.788</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.675</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director.name</th>\n",
       "      <td>0.639</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.601</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person.name</th>\n",
       "      <td>0.655</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.603</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.name</th>\n",
       "      <td>0.807</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.797</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.753</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.724</td>\n",
       "      <td>1091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          p      r      f     s\n",
       "country.name          0.629  0.710  0.667    62\n",
       "movie.location        0.000  0.000  0.000     7\n",
       "award.ceremony        0.714  0.714  0.714     7\n",
       "award.category        1.000  0.000  0.000     2\n",
       "director.nationality  1.000  0.000  0.000     1\n",
       "character.name        0.667  0.267  0.381    15\n",
       "movie.gross_revenue   0.000  0.000  0.000     5\n",
       "movie.genre           0.963  0.722  0.825    36\n",
       "actor.nationality     1.000  1.000  1.000     1\n",
       "movie.type            1.000  0.000  0.000     4\n",
       "movie.release_region  1.000  0.000  0.000     4\n",
       "movie.language        0.786  0.638  0.704    69\n",
       "rating.name           0.935  0.951  0.943    61\n",
       "movie.star_rating     1.000  0.000  0.000     1\n",
       "producer.name         0.692  0.616  0.652    73\n",
       "movie.release_date    0.412  0.483  0.444    29\n",
       "actor.name            0.730  0.675  0.701    80\n",
       "actor.type            1.000  1.000  1.000     2\n",
       "movie.subject         0.788  0.591  0.675    44\n",
       "director.name         0.639  0.568  0.601    81\n",
       "person.name           0.655  0.559  0.603    34\n",
       "movie.name            0.807  0.786  0.797   473\n",
       "total                 0.753  0.697  0.724  1091"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs = read_corpus_conll('tst.conll')\n",
    "hyps = read_fst4conll('w2wt_wt.wt2.out', split=True)\n",
    "\n",
    "results = evaluate(refs, hyps)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Full $\\lambda_{W2WT_{U}}$\n",
    "- Implement $\\lambda_{W2WT_{U}}$ using 'full' input and output symbol tables (`isyms.txt` and `osyms.txt`)\n",
    "- Test the pipeline: $\\lambda_{W} \\circ \\lambda_{W2WT_{U}} \\circ \\lambda_{SCLM_{2}}$\n",
    "    - Observe the issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Compare each pipeline in terms of:\n",
    "    - size of input symbol table\n",
    "    - size of output symbol table\n",
    "    - size (number of arcs) of $\\lambda_{W2T}$\n",
    "    - size of $\\lambda_{*LM}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Common Improvements\n",
    "\n",
    "- Training an ngram language model on data that contains tags only (i.e. $\\lambda_{*LM}$) has one __big issue__: the out-of-span tag (`'O'`) is very frequent, consequently, there is not enough context to learn a good ngram model. \n",
    "- Joint modeling of words and tags, i.e. $\\lambda_{SCLM}$, on the other hand, has a very specific (and less frequent context).\n",
    "- There are two common enhancements to these models:\n",
    "    - removing out-of-span tag `'O'` from the $\\lambda_{*LM}$ to provide context for other tags\n",
    "    - generalization of input into __classes__, i.e. $\\lambda_{G}$, so that the data is less sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Generalization (Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Th Language Understanding pipeline (as presented during the lectures) is \n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{G} \\circ \\lambda_{W2T} \\circ \\lambda_{*LM}$$\n",
    "\n",
    "The function of $\\lambda_{G}$ is this pipeline is to *generalize* the input, reducing sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "In Natural Language Processing it is common to __normalize__ (pre-process) the input data to reduce sparsity (e.g. [textacy](https://chartbeat-labs.github.io/textacy/build/html/api_reference/text_processing.html))'s pre-processing). \n",
    "\n",
    "The __normalization__ replaced all members of the __infinite set__ with a single __unique token__ with respect to a __common pattern__. It is not possible to learn a good model for each possible number, for instance.\n",
    "\n",
    "- The example \"entities\" that have common pattern are:\n",
    "    - numbers\n",
    "    - emails\n",
    "    - url\n",
    "    - phone numbers\n",
    "    - credit card numbers\n",
    "    - etc.\n",
    "\n",
    "These \"entities\" are generally captured using __regular expressions__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lookup Tables\n",
    "Lookup tables provide a convenient way to generalize members of __large__ and __known set__ of entities. The common examples are *cities*, *countries*, *airport codes*, *movie names*, etc.\n",
    "Even though these sets are potentially infinite, the lists of cities and movie names are generally available as external __Knowledge Bases__, and it is possible to check membership of a token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [(Named) Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)\n",
    "> Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "\n",
    "__NER__ also covers entities that are covered by *normalization*, as it is a matter of approach (regex vs. sequence labeling).\n",
    "There are several NLP tasks that fall under this category, specified with respect to the type of entity:\n",
    "    - TIMEX - temporal expressions\n",
    "    - ENAMEX - named entities \n",
    "    - NUMEX - numerical expressions\n",
    "    - etc. (e.g. protein names in BioMedical Domain)\n",
    "\n",
    "The task is similar to Concept Tagging, with an __important__ differences: \n",
    "- the same entity (from NER perspective) may belong to different classes in the target domain: \n",
    "    - e.g. in `NL2SparQL4NLU`: `actor.name`, `producer.name`, `director.name` are subclasses of a `PERSON`\n",
    "\n",
    "Consequently, the output of such systems could be used as input for Concept Tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Lab\n",
    "- Implement number generalization to map all numerical expressions in input to `<num>`\n",
    "- Evaluate the pipeline with this step\n",
    "- Observe sizes of input and output symbol tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
